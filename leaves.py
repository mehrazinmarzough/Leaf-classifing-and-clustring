# -*- coding: utf-8 -*-
"""leaves.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BQnqYXHh2RU_-JoD0IuUoCVJOT-BtEsP
"""

import pandas as pd
df = pd.read_csv("new_data.csv")
df.drop('1',inplace=True,axis=1)
df

from sklearn.model_selection import train_test_split

train , test = train_test_split(df, test_size=0.2, random_state=200)
train

# d = train.corr()
# for col in d.columns:
#   if col == '0':
#     continue
#   x = 0
#   for col2 in d.columns:
#     if col != col2:
#       x += np.abs(d.loc[col,col2])
#   x /= d.shape[1]
#   if x < 0.04:
#     train.drop(col, axis = 1, inplace=True)
#     test.drop(col, axis = 1, inplace=True)
# print(train.shape)

test_y = test['0']
test.drop('0', axis = 1, inplace=True)

whisker_width = 5

for col in train.columns:
  if col == '0':
    continue
  Q1 = train[col].quantile(0.25)
  Q3 = train[col].quantile(0.75)
  IQR = Q3 - Q1
  low_bound = Q1 - whisker_width * IQR
  high_bound = Q3 + whisker_width * IQR
  for i in train.index:
    if train.loc[i, col] < low_bound or train.loc[i, col] > high_bound:
      train.drop(i, inplace=True)

print(train.shape)

train_y = train['0']
train.drop('0', axis = 1, inplace=True)

import numpy as np
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(train)

train = scaler.transform(train)
test = scaler.transform(test)

train = pd.DataFrame(train)

test = pd.DataFrame(test)

# # prompt: pca for train and test

# from sklearn.decomposition import PCA

# pca = PCA(n_components=4)
# pca.fit(train)

# train = pca.transform(train)
# test = pca.transform(test)

# # prompt: lda for train and test

# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# lda = LinearDiscriminantAnalysis(n_components=2)
# lda.fit(train, train_y)

# train = lda.transform(train)
# test = lda.transform(test)

# from sklearn.neighbors import KNeighborsClassifier

# knn = KNeighborsClassifier(n_neighbors=7)

# knn.fit(train, train_y)

# test_y_pred = knn.predict(test)

# print("Accuracy:", accuracy_score(test_y, test_y_pred) * 100, "%")

# model = DecisionTreeClassifier(criterion="entropy", max_depth=9, min_samples_split=4)

# model.fit(train,train_y)

# predictions = model.predict(test)

# accuracy = accuracy_score(test_y, predictions)
# print("Accuracy:", accuracy)

# from sklearn.ensemble import RandomForestClassifier

# random_forest = RandomForestClassifier(n_estimators=70, criterion="entropy", max_depth=14, min_samples_split=5)

# random_forest.fit(train, train_y)

# predictions = random_forest.predict(test)

# accuracy = accuracy_score(test_y, predictions)
# print("Accuracy:", accuracy)

# from sklearn.ensemble import AdaBoostClassifier

# ada_boost = AdaBoostClassifier(n_estimators=100, learning_rate=0.1)

# ada_boost.fit(train, train_y)

# predictions = ada_boost.predict(test)

# accuracy = accuracy_score(test_y, predictions)
# print("Accuracy:", accuracy)

# # prompt: bayes classifier

# from sklearn.naive_bayes import GaussianNB

# bayes = GaussianNB()

# bayes.fit(train, train_y)

# predictions = bayes.predict(test)

# accuracy = accuracy_score(test_y, predictions)
# print("Accuracy:", accuracy)

# prompt: SVM

from sklearn import svm
from sklearn.metrics import accuracy_score

clf = svm.SVC(kernel='linear')
clf.fit(train, train_y)

predictions = clf.predict(test)

accuracy = accuracy_score(test_y, predictions)
print("Accuracy:", accuracy)

# # prompt: 2 layer MLP with keras

# from keras.models import Sequential
# from keras.layers import Dense

# model = Sequential()
# model.add(Dense(2515, input_shape=(2514,), activation='relu'))
# model.add(Dropout(0.5))
# model.add(Dense(37, activation='softmax'))

# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# model.fit(train, train_y, epochs=100, batch_size=32)

# test_loss, test_acc = model.evaluate(test, test_y)
# print('Test accuracy:', test_acc)